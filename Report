\documentclass{article}
\usepackage{svg}
\usepackage[english]{babel}
\usepackage{scrextend}
\usepackage{amsmath}
\usepackage{underscore}
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{hyperref}


\newcommand{\subhead}[1]{\vspace {0.04in}\noindent{\textbf{#1.}}}

\title{Source Code Vulnerability Detection through Machine Learning}
\author{Fengshou Xu,RuiHeng Tan, Zhong Tao, \\ Supervisor: Paria Shirani}
\date{}

\begin{document}
\maketitle

\begin{abstract}
In the cybersecurity field, automated vulnerability detection is crucial for preventing attacks on the vulnerability of the source code. This research explores the effectiveness of defective code vulnerability detection via three GNN models, which are DeepGCN, GraphCodeBERT and GraphGPS. The models will attempt to analyze various types of source code and datasets. Due to the limited time and resource, only the GraphCodeBERT model was able to obtain results within the designated semester. The results indicate the accuracy and effectiveness of GraphCodeBERT in vulnerability detection, and also some defects. This report also discusses the challenges faced by the other two models in the project and suggested directions for future works.
\end{abstract}

\section{Introduction} \label{sec:introduction}

Code vulnerabilities is one of the most significant threat in the field of information security. The traditional analysis methods are not sufficient for massive and accurate vulnerability detection. Graph Neural Networks (GNNs) have demonstrated strong performance in vulnerability detection, making their application in code vulnerability detection a popular research topic. This project is focused on analyze, testing and modify three Graph Neural networks(GNNs) models in the vulnerability detection field. The three models are Deep Graph Convolutional Networks(DeepGCN), CodeGraphBERT, GraphGPS. Our group spent one semester, spanning four months, reading through related essays, understanding the basic structure, running test datasets, and attempting to modify the source code to develop a working model for source code vulnerability detection. This report will introduce the features, performance, and effectiveness of each model.

\section{Background} \label{sec:background}


Vulnerability detection techniques have evolved with the development of computer science. Graph Neural Networks (GNNs) utilize the graphical structure of data for feature abstraction and analysis, demonstrating impressive power in domains like natural language processing. In the domain of code vulnerability detection, transforming code into a graph format allows for capturing the semantic and structural information of the program. DeepGCN enhances representational power through deep graph convolutional networks. GraphCodeBERT combines the BERT architecture from natural language processing with graph structure for code processing and understanding. GraphGPS utilizes both local feature processing and a global attention mechanism to strengthen the representation of node relationships and features. These three models have shown success in other tasks, except for code vulnerability detection. This research will attempt to produce successful results in code vulnerability detection.


\section{Source Code Vulnerability Detection} \label{sec:approach}


\subsection{Deep Graph Convolutional Networks} \label{sec:DeepGCN}
\subsubsection{What is deepGCN?}
DeepGCN stands for deep convolutional network. GCN(Graph convolutional network) is a special type of convolutional neural network where instead of structured euclidean data, it takes in irregular structured data such as graphs and point clouds, hence the name. It is used in detecting pedestrians on the streets in self-driving cars, models proteins for drug discovery, enhances the predictions of search engines and so on and so forth. But traditional GCNs can’t go lower than a few layers, namely 3-4 layers, and performance degrades the further down it goes. But authors of the research paper "DeepGCNs:Can GCNs Go as Deep as CNNs?" managed to solve this problem. According the paper, GCN borrowed some of the orthogonal tricks from its predecessor CNN, effectively solving the problem and in the process, creating deepGCN. The methods adapted from CNN are:residual learning, dense connection and dilated aggregation. The deepGCN model utilized in this project had 56 layer.

\subsubsection{Algorithm}
1.Update Function
\[G_{l+1} = F(G_l,W_l) = Update(Aggregate(G_l,W_l^{agg}),W_l^{update})\]
\textit{G\textsubscript{l}} is the input graph at the l-th layer and \textit{G\textsubscript{l+1}} is the output of that layer after weights have been applied to it. The \textit{Aggregate(G\textsubscript{l},W\textsubscript{l}\textsuperscript{agg})} is responsible for combining the node features and other relevant information from graph \textit{G\textsubscript{l}} based on the aggregated weights of the surrounding nodes \textit{W\textsubscript{l}\textsuperscript{agg}}. After aggregation, the update function is applied, and it updates the current node features based on the aggregated information from the adjacent nodes. The weights \textit{W\textsubscript{l}\textsuperscript{update}} are used to modify the current node features.\\  
2.Residual Learning
\[G_{l+1} = H(G_l,W_l) = F(G_l,W_l) + G_l = G_{l+1}^{res} + G_l\]
\indent Residual learning utilizes residual connections. Residual connection is made up of multiple residual blocks. In a traditional neural network, each node receives input from the nodes in the previous layer with weights applied to them. However, in residual connection, the input from the previous layer bypasses all the in-between layers until it reaches the destination layer, once there it perform vertex addition with the destination layer to obtain the new graph.\\ 
\indent In order to learn the underlying mapping \textit{H}, the graph convolution operation \textit{F} first takes in graph \textit{G\textsubscript{l}} and weights \textit{W\textsubscript{l}} as inputs. Instead of using the output of \textit{F} as the new output graph from this layer \textit{G\textsubscript{l+1}}, the original input \textit{G\textsubscript{l}} is added to it. The result of \textit{F}, \textit{$G_{l+1}^{\text{res}}$}is added to the original input graph \textit{G\textsubscript{l}} as shown on the left side of the equation. 

\subsubsection{Architecture}
    \begin{figure}
        \centering
        \includegraphics[width=0.5\linewidth]{Screenshot 2024-04-22 160744.png}
        \caption{deepGCN architecture}
        \label{fig:deepGCNArchtecture}
    \end{figure}
\begin{enumerate}
    \item \textbf{GCN Backbone Block}\\
    The GCN backbone block is responsible for feature transformation of the input graph. The input graph is processed through this block, which applies graph convolution to extract features from the graph structure. The GCN uses K-NN(K nearest neighbours)algorithm to aggregate features from the current node's neighbours. The implementation of deepGCN used in this project is ResGCN, which is a GCN with residual learning applied to it in the backbone block.
    \item \textbf{GCN Fusion Block}
    The fusion block is responsible for global feature generation and fusion. It consists of a series of 1x1 convolutional layer(1x1 conv), which means that for each feature map extracted, a 1x1 convolutional filter is applied to each node. The number f=1024 indicates the number of filters or feature maps outputted by this convolutional layer. The max pooling layer then takes the outputted feature maps and reduces the data's dimension, resulting in a compressed representation of the input data. 
    \item \textbf{MLP Prediction block}
    The MLP prediction block is responsible for node-wise label prediction. This multi-layer perceptron block is composed of a series of fully connected 1x1 conv layers, each with different numbers of filters. The block maps the extracted features from the the GCN backbone and fusion block to the final output predictions. 
\end{enumerate}
\subsubsection{Environment}
\begin{itemize}
    \item \textbf{Python version \(\geq\, 3.7\)}
    \item \textbf{Pytorch version \(\geq\,1.9.0\)}
    \item \textbf{Pytorch_geometric version \(\geq\, 1.6.0\)}
    \item \textbf{CUDA version:10.2}
    \item \textbf{Setup:}Run command \textit{source deepgcn_env_install.sh}. This command executes the provided .sh script "deepgcn_env_install.sh" which contains a series of pip and conda statements that downloads and updates the requirements. The script can only be ran under Linux environment. 
    \item \textbf{Training:} Training is done by running command \textit{"python main.py --phase train --n_blocks 28 --block res --data_dir /path/to/modelnet40"} where "/path/to/modelnet40" needs to be replaced with the actual path to the dataset. The point-cloud dataset provided in the example is automatically downloaded from the source. For the purpose of this project, it will be replaced with the path to our own dataset. 
    \item \textbf{Testing:} Testing is done by running command \textit{python main.py --phase test --n_blocks 28 --block res  --pretrained_model /path/to/pretrained_model --data_dir /path/to/modelnet40} the path behind "--data_dir" needs to be replaced with the actual path to the modelnet40 dataset. For the sake of time, the testing was done on the pretrained_models provided by the authors. 
\end{itemize}
The testing were done on the graham cluster of AllianceCan. 
\subsubsection{Main problems encountered}
\begin{enumerate}
    \item \textbf{Enviroment:} The testing and training of the model requires two NVIDIA Tesla V100 GPU. This was solved with the use of allinaceCan clusters.
    \item \textbf{Downloading training set:} The model requires an internet connection to download the training set from the source. Since allianceCan computer does not support a internet connection, the training set was unable to be downloaded. This was solved with uploading the dataset manually via Cyberduck.
    \item \textbf{Input handling:} The model was designed to take in point clouds as the input layer. Point clouds have a completely different structure to the provided input JSON labelled graphs. The model code needed to be modified extensively either to convert the input JSON labelled graph to a point cloud or change the input layer to JSON labelled graph by converting the input to a torch_geometric data object. The later option was chosen because the first option contains the potential risk of breaking the code architecture. 
\end{enumerate}
\subsection{Code Representations with Data Flow} 
\subsubsection{What is GraphCodeBERT?}
GraphCodeBERT is one of the pre-trained models for programming languages that utilizes the power of DFG (Data Flow Graph), which enhances its capability to understand code semantics. It has four main research branches: Clone Detection, Code Search, Code Refinement, and Code Translation.
The Key of GraphCodeBERT is DFG, Data Flow Graphs, which is one of the graph representation to indicates data dependencies of one program. 
There several main feature of DFG:
\begin{enumerate}
    \item Capture the data dependency: DFG will expose the data flow direction in one program, as is how one operation's output becomes other operation's input, which helps understanding the behavior of a program
    \item Ignore control flow: DFG doesn't concern the control flow of a program like CFG (Control Flow Diagram). It only targeting the flow of data, which makes it a reliable tool to analysis data dependency
    \item Language independent: DFG is a general presentation method, which can be applied to each program language, which increase its applicability
\end{enumerate}
My research focus on vulnerability detection by using GraphCodeBERT, specifically its Clone Detection feature. This part of GraphCodeBERT effectively identifies functionally similar code snippets by utilizing Data Flow Graphs (DFGs) to better understand of code semantics, which I adapt to be able detect the vulnerabilities in codebases.

\subsubsection{Algorithm of model strcture}
According to the GraphCodeBERT author's arxiv research paper \cite{guo2021graphcodebert}, there are some important points of the model architecture:

First, the input sequence \emph{X} will be converted to input vectors $H^0$. Then the input vectors $H^0$ will be transfer to N transformer layer to produce the final contextual representations $H^n$ where $H^n = transformer_n(H^{n-1})$ . The \emph{H} here imply the "Hidden State", and each transformer layer will produce a hidden state, which used to capture the contextual representations of the input sequence. Each transformer layer contains an architecturally identical transformer that applies a multi-headed self-attention operation (Vaswani et al., 2017 \cite{vaswani2023attention}) and followed by a feed forward layer over the input $H^{n-1}$ in the n-th layer:
    \begin{equation}
        G^n = LN(MultiAttn(H^{n-1}) +  H^{n-1}
    \end{equation}
    \begin{equation}
        H^n = LN(FFN(G^n) + G^n)
    \end{equation}

The \emph{MultiAttn} represents the multi-head self-attention mechanism, \emph{FFN} stands the two layers feed forward network, and \emph{LN} is a layer normalization operation. We could simply interpret these two formula as each transformer layer will apply the multi-head self-attention mechanism and feed forward transformation in order to generate the new hidden state $H^n$. The hidden state $H^n$ captured the input sequence's contextual representations at n-th transformer layer. The hidden state can learn much more abstract feature representation through the increasing of transformer layer. The output $H^n$ of the last transformer layer will be used for the down stream task, for example clone detection and vulnerability detection.

Second, if we dive into more details about the transformer layer, then we have:
\begin{equation}
    Q_i = H^{n-1}W_i^Q , K_i = H^{n-1}W_i^K, V_i = H^{n-1}W_i^V
\end{equation}
\begin{equation}
    head_i = softmax(\frac{Q_iK-I^T}{\sqrt{d_k}}+M)V_i
\end{equation}
\begin{equation}
    \hat{G^n} = [head_1;...;head_u]W^O_n
\end{equation}

Let's understand these formula: first, the formula (3) interpret the process of how to generate the output $\hat{G^n}$ by applying the multi-head self-attention mechanism at transformer layer n. To be more specific: 
\begin{enumerate}
    \item $Q_i = H^{n-1}W_i^Q$ means multiply the last layer's hidden state $H^{n-1}$ and the query matrix $W_i^Q$ to get the query vector $Q_i$
    \item $K_i = H^{n-1}W_i^K$ means multiply the $H^{n-1}$ and the key matrix $W_i^K$ to get the key vector $K_i$
    \item $V_i = H^{n-1}W_i^V$ means multiply the $H^{n-1}$ and the value matrix $W_i^V$ to get the value vector $V_i$
\end{enumerate}
where \emph{i} is the index of attention head, and transformer layer usually use multiple attention heads to capture different attention mode.
Then, the formula (4) shows the procedure to calculate the weight of each attention head by using the \emph{softmax} function. The \emph{$Q_iK^T$} is the similarity of of query vector and the key vector, \emph{d_k} is the dimension of the key vector, and \emph{M} is a mask matrix to block invalid position. The \emph{softmax} function transfer the similarity score to the attention weight, and multiply with vector V_i to get the output head_i of i-th attention head.
At last, formula (5) concatenate all attention head's output to get the output \emph{$\hat{G^n}$} of multi-head self-attention. Here the \emph{u} is the number of attention head, W_O is a linear projection matrix which project the concatenated vector into the model's hidden state dimension.
We can say this process is the model calculates the correlation of each position of the input sequence and other position from different aspect through multiple attention heads. The output \emph{$\hat{G^n}$} will be proceed through a feed forward network and a normalization layer in order to produce the final n-th transformer layer's output \emph{$H^n$} as we seen in formula (1) and (2)

\subsubsection{Algorithm of the core: Graph-Guided Masked Attention}
GraphCodeBERT introduces the DFG into its model to capture the dependency relationships in code, which can help improve the model's code understanding ability. To incorporate the graph structure into the Transformer , they defined a \emph{"graph-guided masked attention function"} \cite{guo2021graphcodebert}.

In traditional self-attention mechanisms, each query attends to all keys. However, in \emph{"graph-guided masked attention"}, the mask function ensures that the query only pays attention to the keys directly related to it in the graph. If a key and a query have no direct connection, their attention score will be negative infinity, which will make the weight become 0 after applying the softmax function.

For code representation, the graph structure shows the dependency between variables. If the corresponding nodes of two variables in the source code have a direct edge or are the same node, these variables are considered dependent and should be included in the self-attention calculation.

By using graph information, we can construct an attention matrix and mask the keys that should be noticed by the current query. The source code markers \emph{[CLS]} and \emph{[SEP]} are always considered in the attention calculation because they carry global sentence-level and paragraph-level information, which is crucial for understanding the entire code snippet. Other tokens can only be considered when they are related to the query in the graph.

Formally, the \emph{graph-guided masked attention matrix} can be described as the mask matrix \emph{M}:

\begin{equation}
M_{ij} =
\begin{cases}
0 & \text{if } q_i \in {\text{[CLS]}, \text{[SEP]}} \text{ or } q_i, k_j \in W \cup C \text{ or } \langle q_i, k_j \rangle \in E \cup E', \\
-\infty & \text{otherwise}
\end{cases}
\end{equation}

When $M_{ij}$ is 0, it means the $i$-th query needs to consider the $j$-th key; when $M_{ij}$ is negative infinity, the $i$-th query does not need to consider the $j$-th key.

\subsubsection{Adapting model to vulnerable detection}
To adapt the model to the vulnerability detection task, I modified the input of the model. It now requires only one input instead of two, and one label (0 or 1, where 0 indicates not vulnerable, and 1 indicates vulnerable) that specifies whether the given input contains a vulnerability. Additionally, I implemented a new DFG function to extend GraphCodeBERT's capability to handle C++ code. I completed this new DFG by learning from the original DFG_java function. The reason I chose to borrow from the Java version implementation is that Java and C++ are both object-oriented programming (OOP) languages that share many similar concepts, making the original DFG_java function a good starting point. With these modifications, the GraphCodeBERT model is now ready for C++ code vulnerability detection.

\subsubsection{Training Model}
After transform the model into vulnerability detection, we need to train this model with new C++ code data. Due to the limited time and resources available to me as a student, I couldn't generate a high-quality labeled C++ vulnerability code dataset within the span of a semester, or even in the last month and a half. Creating a usable dataset from scratch was not feasible given the constraints. Lucky, Our supervisor's assistant sent me one dataset that based on the history code of open source project "qemu" and "ffmpeg", which has roughly 27k entry with label ; so that I could start to train the model.

\label{sec:CodeGraphBERT} 

\subsection{Graph Transformer} \label{sec:GPS} 

\subsubsection{What is GPS graph transformer?}

The GPS Graph transformer stands for the most general, powerful, scalable (GPS) graph
Transformer. This research is down by the group of Ladislav Rampášek, Mikhail Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini\cite{rampasek2022GPS}.  This group submitted a research paper titled "Recipe for a General, Powerful, Scalable Graph Transformer", which was included in the proceedings of NeurIPS 2022. The GPS Transformer provides a way to build a scalable graph Transformer with linear complexity and applying it's architecture to a variety of benchmarks to evaluate its effectiveness and scalability.\\

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{img/GraphGPS.png}
    \caption{GraphGPS layers}
    \label{fig:GPSLayer}
\end{figure}

It has 3 main ingredients: positional/structural encoding, local message-passing mechanisms, and global attention mechanisms. These ingredients enable the GPS to capture local and global graph features more effectively, and also avoid the over-smooth and over-squashing problem happened in other graph neural network.\\

Positional encoding adds information about the order and relative positions of elements by order to the model, and enabling the model to understand the order's structure and abstract the meaning based on the arrangement of these elements. Structural encoding allows the node to include information related substructures, global network structure and the similarity between two different nodes. The essay also organize PE and SE into 3 categories: local, global and relative in order to further specifies the expected roles in this model.\\

Message-passing mechanism(MPNN) is a specific type of Graph Neural Network which using a message-passing structure to update the representation of nodes based on the information gathered from their neighbours. In this model, MPNNs model is based on PyTorch Geometric and its related components.\\

Global attention mechanism is supported by graph transformer. Basically, the feature goes into MPNNs layer, after normalization, graph transformer will process and abstract global features to complement local features.

\subsubsection{Algorithm}

\begin{enumerate}
    \item \textbf{Update Function:}
    \[
    X^{t+1}_M, E^{t+1} = \text{GPS}'(X'^{t}, E'^{t}, A)
    \]
    The inputs are the modified node features ($X'^{t}$), edge features ($E'^{t}$). The adjacency matrix ($A$), and the outputs are the features of nodes ($X^{t+1}_M$) and edges ($E^{t+1}$) for the next timestep.

    \item \textbf{Message Passing Neural Network (MPNN):}
    \[
    X^{t+1}_M = \text{MPNN}^{\phi}(X'^{t}, E'^{t}, A)
    \]
    Node features are updated  by using a Message Passing Neural Network. The MPNN processes the current node features, edge features, and the adjacency matrix.

    \item \textbf{Global Attention:}
    \[
    X'^{t+1} = \text{GlobalAttn}^{\theta}(X'^{t})
    \]
    Node features are evaluated and updated according to their importance and context of the entire graph by using a global attention mechanism.

    \item \textbf{Multi-Layer Perceptron (MLP):}
    \[
    X^{t+1} = \text{MLP}^{\psi}(X^{t+1}_M + X'^{t+1})
    \]
    The final node features are processed by a Multi-Layer Perceptron. This MLP processes the combined features from the MPNN and the global attention mechanism. After processing, the Multi-Layer Perceptron generates updated features for the next timestep by integrating local and global features about the nodes.
\end{enumerate}

 
\subsubsection{Environment}

The operations are conducted on the Alliancecan.ca server (Compute Canada, Linux environment).

\begin{itemize}
    \item \textbf{Python Version:} 3.10 (default)
    \item \textbf{Python Virtual Environment:} Created using \texttt{python -m venv my\_venv}. The GPS documentation suggests running the code under Conda; however, the Alliancecan server does not support Conda installations. Thus, a Python virtual environment is used instead. Remember to activate the virtual environment when submitting a .sh file by using the 'source' command.
    \item \textbf{PyTorch Version:} 1.13 (default)
    \item \textbf{PyTorch Geometric Version:} 2.2 (default)
    \item \textbf{CUDA Version:} cu117 (default)
    \item \textbf{Torch Sparse:} 0.6.18. Install with the command \texttt{pip install torch\_sparse==0.6.18 -f https://data.pyg.org/whl/torch-1.13.1+cu117.html}. This component is required to run GPS, although it is missing from the documentation. The provided URL contains many components that may be required by GPS; use them as needed.
    \item \textbf{Model Load OpenBabel:} This model cannot be installed on Alliancecan but can be loaded instead.
\end{itemize}

\textbf{Additional Notes:}
\begin{itemize}
    \item The \textbf{Narval} node may cause errors (e.g., issues converting SMILE strings).
    \item The \textbf{Cedar} node is preferred for operations.
    \item The W\&B probably cause an unauthorized access error. Therefore, wandb.use False may avoid this problem.
\end{itemize}

All other components not mentioned should follow the installation steps provided by the GPS GitHub documentation.


\subsubsection{Why Graph Transformer good for  vulnerability detection?}

\textbf{Message Passing Mechanism:} The GraphGPS model combines context information from other neighbouring nodes, which is able to analysis the broader structure and the flow of the program.

\textbf{Masking and Attention Mechanisms(Selective Focus):} The attention mechanisms allow GraphGPS to focus on the most relevant parts of the code. Therefore, the ability of detection of critical vulnerabilities can be improved.

\textbf{Generalization:} The GraphGPS abstracting basic patterns, make their application more generalized for diverse and various code structure vulnerability detection.

\textbf{Representational Power:} The GraphGPS wraps both the local and global code properties within their code representations, which can capture complicated dependencies and behaviours.

\subsubsection{Main Problems Encountered}

\begin{enumerate}
    \item \textbf{Data Handling Logic:} Main challenges in this project, including correctly loading, processing, storing graph data, and node features and edge connections. This part not finished since this part need more time to test and debug.
    \item \textbf{Module Path Issues:} Frequent issues with incorrect module and dataset loading paths. This often causes failures when importing and modifying modules and datasets.
    \item \textbf{Folder Structure and Namespace Conflicts:} My modification of specific modules often cause conflicts with existing GraphGPS and PyTorch Geometric files, particularly with relative paths during imports. In order to make testing progress, absolute paths are used during testing; however, this is may need correction in the future.
    \item \textbf{Python Environment Configuration:} Error of the system path (PYTHONPATH) caused issues with the Python environment not finding modules correctly.
\end{enumerate}
  



\subsection{Evaluation}
\subsubsection{Evaluation of GraphCodeBERT}
I trained and evaluated on the given dataset mention above. I followed the original GraphCodeBERT dataset arrangement, split the dataset into train(80\%), evaluation(10\%), and test(10\%) 3 parts. As the core of GraphCodeBERT model remains unchanged, I can reuse the original evaluation script with some minor modifications to evaluate the performance of the new, modified GraphCodeBERT model. The metrics for evaluating this model performance are: Recall, Prediction(Accuracy), and F1 score. Please refer to to the GitHub repository: \href{https://github.com/Fengshou-Xu/GraphCodeBERT_Cpp_Vulnerability_Detection/tree/main}{GraphCodeBERT_Cpp_Vulnerability_Detection} for more details.

\subsection{Experimental Setup}
\subsubsection{Experimental Setup for GraphCodeBERT}
I conduct the training and evaluation on both Google Colab with A100-40G GPU and the \emph{cedar} cluster with 4-V100-16G GPU. The environment requirement for modified GraphCodeBERT are:
\begin{enumerate}
    \item Python \(\geq\, 3.8\)
    \item torch
    \item transformers
    \item tree_sitter
    \item scikit-learn
\end{enumerate}
Please also refer to to the GitHub repository: \href{https://github.com/Fengshou-Xu/GraphCodeBERT_Cpp_Vulnerability_Detection/tree/main}{GraphCodeBERT_Cpp_Vulnerability_Detection} for more details.

\subhead{Dataset} \label{sec:dataset}
We conduct our experiments using following datasets:
\begin{itemize} 
    \item \textbf{Dataset I:} modelnet40(point-cloud dataset used to verify the overall accuracy and mean intersection over union of deepGCN)
    \item \textbf{Dataset II:} a dataset that based on the history code of open source project "qemu" and "ffmpeg" (for GraphCodeBERT)
    \item \textbf{Dataset III:} ZINC(used to verify the effectiveness of GraphGPS) 
\end{itemize}

\subsection{DeepGCN Accuracy Results} \label{sec:DeepGCN-res}
Due to the limited amount of time, only ResGCN which is the GCN model with residual learning applied to it was able to be tested. The result of testing the dataset on the task of point cloud segmentation is shown below, the result matches the result indicated in the paper. 
\begin{figure}
    \centering
    \includegraphics[scale=0.95]{Screenshot 2024-04-23 144812.png}
    \caption{Result of ResGCN-28(28 layers)}
    \label{fig:deepGCN1}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.95]{Screenshot 2024-04-23 144833.png}
    \caption{Result of ResGCN-14}
    \label{fig:deepGCN2}
\end{figure}
\subsection{CodeGraphBERT Accuracy Results} \label{sec:CodeGraphBERT-res}
The performance of CodeGraphBERT model is: 'Recall': 0.78, 'Prediction': 0.77, 'F1': 0.77, which should be an acceptable result for vulnerability detection.
\subsection{GPS Transformer Accuracy Results} \label{sec:GPS-res}
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{img/Screenshot2.png}
    \caption{Result of GraphGPS}
    \label{fig:resultofGPS}
\end{figure}

According to the essay, the Performance of GraphGPS on Various Datasets, the ZINC should get Mean Absolute Error(MAE) = 0.070 \(\pm\; 0.004\).

In my testing, the result of ZINC MAE is 0.0727, which meets the description in essay.


\section{Conclusion and Future Work}
In conclusion, this project explored the application of various models for detecting vulnerabilities in C++ code. Each model presented unique characteristics, and through customization and adaptation, we somehow managed to achieve effective identification of potential vulnerabilities. Our future work will focus on improving the model performance, exploring more model variants, and validating model generalizability on broader datasets.

\bibliography{refs}
\nocite{*}
\bibliographystyle{IEEEtran}


\section*{Contribution of the Authors}
This project was collaboratively completed by Authors RuiHeng Tan,Fengshou Xu , and Zhong Tao, each author responds for a specific model and evaluate its applicability in vulnerability detection. Specific contributions are as follows:
\begin{enumerate}
    \item Author RuiHeng Tan conducted research \emph{deepGCN}, his research and findings are detailed in section \ref{sec:DeepGCN}
    \item Author Fengshou Xu investigated the model \emph{GraphCodeBERT}, relevant research methods and result can be found at section \ref{sec:CodeGraphBERT} 
    \item Author Zhong Tao explored the capabilities of \emph{GPSGraph}, his research details listed in section \ref{sec:GPS}
\end{enumerate}
We would like to express our sincere gratitude to supervisor Paria Shirani and her assistant Asmaa Hailane, who provided invaluable guidance and support throughout this project.

\end{document}

